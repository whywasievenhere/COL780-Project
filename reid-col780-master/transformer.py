import torchimport torch.nn as nnfrom einops import rearrange, repeatfrom einops.layers.torch import Rearrangeclass Normal_block(nn.Module):        def __init__(self,dim,fn):        super().__init__()        self.norm = nn.LayerNorm(dim)        self.fn = fn        def forward(self,x,**kwargs):        return self.fn(self.norm(x), **kwargs)    class Feedforward(nn.Module):        def __init__(self,dim,hidden_dim, dropout = 0.):                super().__init__()        self.layer = nn.Sequential( nn.Linear(dim,hidden_dim),                                    nn.GELU(), nn.dropout(dropout),nn.Linear(hidden_dim,dim),                                    nn.dropout(dropout) )            def forward(self,x):        return self.layer(x)    class Attention(nn.Module):        def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):                super().__init__()        self.multi_head_attn = nn.MultiheadAttention(dim_head,heads,dropout)        inner_dim = heads* dim_head        self.to_out = nn.Identity()         if inner_dim != dim:            self.to_out = nn.Linear(inner_dim,dim)                def forward(self,x):        return self.to_out(self.multi_head_attn(x,x,x))    class Transformer(nn.Module):    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):                super().__init__()        self.layers = nn.ModuleList([])        for i in range(0,depth):            self.layers.append(nn.ModuleList([ Normal_block(dim,Feedforward(dim,mlp_dim,dropout)),                                        Normal_block(dim, Attention(dim,heads,dim_head,dropout))]))                def forward(self,x):        for att, ff in self.layers:            x = att(x) + x            x = ff(x) + x        return x        class ViT(nn.Module):        def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):                super().__init()        image_height, image_width  = image_size[0], image_size[1]        patch_height, patch_width  = patch_size[0], patch_size[1]                assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'                num_patches = (image_height // patch_height) * (image_width // patch_width)         patch_dim = channels * patch_height * patch_width        self.embed_patch = nn.Sequential( Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),                                         nn.Linear(patch_dim, dim))                self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))        self.cls_token = nn.Parameter(torch.randn(1,1,dim))        self.dropout = nn.Dropout(emb_dropout)                self.transformer = Transformer(dim,depth,heads,dim_head,mlp_dim,dropout)        self.pool = pool        self.mlp_head = nn.Sequential(nn.LayerNorm(dim),nn.Linear(dim,num_classes))            def forward(self,img):        x = self.embed_patch(img)        b,n,_ = x.shape        cls_tokens = repeat(self.cls_token,'() n d -> b n d', b = b)        x = torch.cat((cls_tokens,x),dim=1)        x += self.pos_embedding        x = self.dropout(x)        x= self.transformer(x)        if self.pool == 'mean' :            x = x.mean(dim=1)         else:            x = x[:, 0]        return self.mlp_head(x)